
%% bare_jrnl.tex
%% V1.3
%% 2007/01/11
%% by Michael Shell
%% see http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.7 or later) with an IEEE journal paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%% and
%% http://www.ieee.org/



% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. IEEE's font choices can trigger bugs that do  ***
% *** not appear when using other class files.                            ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/


%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%
%% File list of work: IEEEtran.cls, IEEEtran_HOWTO.pdf, bare_adv.tex,
%%                    bare_conf.tex, bare_jrnl.tex, bare_jrnl_compsoc.tex
%%*************************************************************************

% Note that the a4paper option is mainly intended so that authors in
% countries using A4 can easily print to A4 and see how their papers will
% look in print - the typesetting of the document will not typically be
% affected with changes in paper size (but the bottom and side margins will).
% Use the testflow package mentioned above to verify correct handling of
% both paper sizes by the user's LaTeX system.
%
% Also note that the "draftcls" or "draftclsnofoot", not "draft", option
% should be used if it is desired that the figures are to be displayed in
% draft mode.
%
\documentclass[journal]{IEEEtran}
\usepackage{blindtext}
\usepackage{graphicx}

% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/tex-archive/macros/latex/contrib/oberdiek/
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 4.0 (2003-05-27) and later if using hyperref.sty. cite.sty does
% not currently provide for hyperlinked citations.
% The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/cite/
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at: 
% http://www.ctan.org/tex-archive/macros/latex/required/graphics/
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found as epslatex.ps or
% epslatex.pdf at: http://www.ctan.org/tex-archive/info/
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage[cmex10]{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics. If using
% it, be sure to load this package with the cmex10 option to ensure that
% only type 1 fonts will utilized at all point sizes. Without this option,
% it is possible that some math symbols, particularly those within
% footnotes, will be rendered in bitmap form which will result in a
% document that can not be IEEE Xplore compliant!
%
% Also, note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithms/
% There is also a support site at:
% http://algorithms.berlios.de/index.html
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/tex-archive/macros/latex/contrib/algorithmicx/




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/


%\usepackage{mdwmath}
%\usepackage{mdwtab}
% Also highly recommended is Mark Wooding's extremely powerful MDW tools,
% especially mdwmath.sty and mdwtab.sty which are used to format equations
% and tables, respectively. The MDWtools set is already installed on most
% LaTeX systems. The lastest version and documentation is available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/mdwtools/


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


%\usepackage{eqparbox}
% Also of notable interest is Scott Pakin's eqparbox package for creating
% (automatically sized) equal width boxes - aka "natural width parboxes".
% Available at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/eqparbox/





% *** SUBFIGURE PACKAGES ***
%\usepackage[tight,footnotesize]{subfigure}
% subfigure.sty was written by Steven Douglas Cochran. This package makes it
% easy to put subfigures in your figures. e.g., "Figure 1a and 1b". For IEEE
% work, it is a good idea to load it with the tight package option to reduce
% the amount of white space around the subfigures. subfigure.sty is already
% installed on most LaTeX systems. The latest version and documentation can
% be obtained at:
% http://www.ctan.org/tex-archive/obsolete/macros/latex/contrib/subfigure/
% subfigure.sty has been superceeded by subfig.sty.



%\usepackage[caption=false]{caption}
%\usepackage[font=footnotesize]{subfig}
% subfig.sty, also written by Steven Douglas Cochran, is the modern
% replacement for subfigure.sty. However, subfig.sty requires and
% automatically loads Axel Sommerfeldt's caption.sty which will override
% IEEEtran.cls handling of captions and this will result in nonIEEE style
% figure/table captions. To prevent this problem, be sure and preload
% caption.sty with its "caption=false" package option. This is will preserve
% IEEEtran.cls handing of captions. Version 1.3 (2005/06/28) and later 
% (recommended due to many improvements over 1.2) of subfig.sty supports
% the caption=false option directly:
%\usepackage[caption=false,font=footnotesize]{subfig}
%
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/subfig/
% The latest version and documentation of caption.sty can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/caption/




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure. The latest version and documentation can be found at:
% http://www.ctan.org/tex-archive/macros/latex/base/



%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/sttools/
% Documentation is contained in the stfloats.sty comments as well as in the
% presfull.pdf file. Do not use the stfloats baselinefloat ability as IEEE
% does not allow \baselineskip to stretch. Authors submitting work to the
% IEEE should note that IEEE rarely uses double column equations and
% that authors should try to avoid such use. Do not be tempted to use the
% cuted.sty or midfloat.sty packages (also by Sigitas Tolusis) as IEEE does
% not format its papers in such ways.


%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley and Jeff Goldberg.
% This package may be useful when used in conjunction with IEEEtran.cls'
% captionsoff option. Some IEEE journals/societies require that submissions
% have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.3.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% For subfigure.sty:
% \let\MYorigsubfigure\subfigure
% \renewcommand{\subfigure}[2][\relax]{\MYorigsubfigure[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat/subfig command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/endfloat/
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.





% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/contrib/misc/
% Read the url.sty source comments for usage information. Basically,
% \url{my_url_here}.

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )

\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{float}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\makeatletter
\def\BState{\State\hskip-\ALG@thistlm}
\makeatother

% correct bad hyphenation here
\begin{document}
\title{Upon the Deanonymization of Bitcoin Transactions}
\author{Yash Patel, Matt Weinberg}

\markboth{MAT Senior Thesis 2018}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
Bitcoin (BTC), previously relegated as a technology residing in the niche of only the most technologically competent of computer scientists, has emerged into the mainstream public in an enormous way. Ever since its explosive rise over the past year, technologists and investors both have sought to understand and exploit its technical limits. As illustrated in Figure \ref{fig:BTC_price}, speculation on the asset of Bitcoin has become an undeniable sector of the modern technology world, especially when viewed from those outside of it. While current speculations are high, many investors and technologists both struggle to understand the fundamental use case of Bitcoin and its underlying technology: blockchain. The speculative bubble is largely the product of vacuous interest pooling into an asset of misunderstood potential, paralleling the financial bubble surrounding the dot-com bubble. 

\begin{figure}
    \label{fig:BTC_price}
    \centering
    \includegraphics[width=0.4\textwidth]{BTC_price.png}
    \caption{The price of one Bitcoin, as marked by Bloomberg. While technologists saw its promise many years prior, Bitcoin's rise in popularity in the general public has led to exponential speculative interest \cite{goldman}.}
\end{figure}

Prior to further discussing Bitcoin's use cases as a whole, it serves a purpose to understand both the technology and underlying technology first. Briefly, Bitcoin is a cryptocurrency, by which we mean a ``digital unit of exchange that is not backed by a government-issued legal tender" \cite{virtual}. The blockchain technology underpinning Bitcoin and other cryptocurrencies, as illustrated in Figure \ref{fig:blockchain}, is ``a solution to the double-spending problem using a peer-to-peer distributed timestamp server to generate computational proof of the chronological order of transactions. The system is secure as long as honest nodes collectively control more CPU power than any cooperating group of attacker nodes" \cite{bitcoin}. Described plainly, blockchain seeks to solve a technical problem, namely that of achieving consensus in a distributed system while preventing double-spending of the asset being exchanged, i.e. arbitrating exchange of a common asset without the need of a trusted central authority. Since the desired end goal is transactions without a trusted third-party, it must be the case that any peer in this network can verify transactions as valid, which has a natural correspondence to the well-known concept of public-key encryption. It, therefore, should come as little surprise that this is how wallets are implemented in Bitcoin and similar cryptocurrencies.

Specifically, as with any public-key encryption system, Bitcoin exchange revolves around participating members having two keys, one public and one private. For the specific use case of Bitcoin, the private key is used to sign any transactions messages from one account to another. In turn, any other account, particularly that of a receiving party, can verify it indeed originated from the claimed party using the public key, as demonstrated in further detail in Figure \ref{fig:}.

\begin{figure}
    \label{fig:address}
    \centering
    \includegraphics[width=0.4\textwidth]{address.png}
    \caption{While public-key encryption is the basic idea behind the Bitcoin protocol, there are many additional details involved in making it a practical system to be used. In particular, many additional hashing functions are used in an effort to making the underlying key hashes more manageable to deal with and store, while meeting sufficient security demands. These details, while interesting in their own right, however, are not of key important for this particular study \cite{public-key}.}
\end{figure}

Incidentally, these public keys serve as what people colloquially refer to as ``wallet." Specifically, ``Bitcoin, by contrast, is designed with pseudonymous identities. Account numbers are public keys of a specific asymmetric encryption system" \cite{laundering}. This notion of Bitcoin being ``pseudonymous" simply refers to the presence of an abstraction layer between real life and those IDs used within the Bitcoin universe.

\begin{figure}
    \label{fig:blockchain}
    \centering
    \includegraphics[width=0.4\textwidth]{blockchain.png}
    \caption{Blockchain, the underlying technology behind Bitcoin and all other cryptocurrencies, is simply a data structure consisting of an extended linked list of hash pointers with metadata. These pieces of metadata, in addition to carrying the actual BTC information of interest, more importantly serve as a piece of data that can be used for efficient verification of double-spending in a distributed system \cite{blockchain-img}.}
\end{figure}

Returning to the discussion of interest, Bitcoin when viewed from this perspective has nothing inherently making it more adept with privacy aside from the abstraction of people to public wallet addresses. Yet, this is seemingly fully comparable to the abstraction imposed by the current banking system of the abstraction between a person's account/routing number and real life identity. The key differences, however, are twofold. In the first place, the abstraction layer instituted by banks are leaky. That is to say, while people generally do not know one another banking numbers, authorities within banks have this association, for they oftentimes require personal details be provided in signing up for a bank account. Bitcoin has no similar gatekeeper with such identifying information by virtue of creating a authority-free protocol, meaning Bitcoin wallets can be procured without the need to link it with personal details. The second distinguishing feature is the ease with which such wallets can be created. In line with the lack of a central authority, there is no barrier in place preventing a single person or entity from creating many Bitcoin wallets. This follows as, in the process of making a wallet, there is \textit{no} way to distinguish whether the creator is someone who has made a wallet or not by the lack of associated identifying attributes. Thus, the properties of privacy that have come to be associated with Bitcoin have been born out of its lack of a central authority holding identifying links. 

Returning to the discussion of the use cases of Bitcoin, many have been born out of this seeming impossibility in linking real-life identity with associated transaction wallets. It, therefore, seems natural that such use cases are generally of some illicit nature, as, if they were not, there would be no benefit of using Bitcoin over a traditional bank, as the former does not provide the typical fraud protections that are instituted by the latter. One government report discussed cryptocurrencies in the following light:

``[Cryptocurrencies] present particularly difficult law enforcement challenges because of their ability to transcend national borders in the fraction of a second, unique jurisdictional issues and anonymity due to encryption. Due primarily to their anonymity, virtual currencies have been linked to numerous types of crimes, including facilitating marketplaces for: assassins, attacks on businesses, the exploitation of children (including pornography), corporate espionage, counterfeit currencies, drugs, fake IDs and passports, high yield investment schemes (Ponzi schemes and other financial frauds), sexual exploitation, stolen credit cards and credit card numbers, and weapons" \cite{virtual-currency}. 

To point to one particular example, the Silk Road, an ``online market...[that] specialized in `black market' goods, such as pornography, weapons or narcotics...corresponded to about 4.5\% of all transactions occurring in exchange" during the early days of Bitcoin \cite{silk}. While Bitcoin's integration into the mainstream has greatly exploded since these statistics were measured, the fact remains that a significant portion of Bitcoin's trading activity lie in either illicit or speculation markets. It, therefore, becomes a question of interest whether such illicit activity can be tracked and retraced. By this, we mean to say, produce a map that links wallet addresses to real-life identities. What is meant by ``real-life identities" is somewhat ill-defined, as to what level of granularity we wish to resolve. For example, a group of addresses could be resolved as ``John Smith" or ``Coinbase" assuming John Smith has his wallet handled through Coinbase. Briefly, Coinbase performs two primary purposes: serving as a platform of exchange for cryptocurrencies and providing a simply way to set up corresponding wallets. These wallets, however, are not ``owned" by the end user in the traditional sense, since the private key is owned by Coinbase. Internally, they simply have a map linking a particular private key to the user's account meaning any activity performed by the user on Coinbase is simply relayed to the Coinbase API, which serves as a ``middleman" to the end Bitcoin application layer. The main relevant information is that Coinbase serves as a centarl manager of the accounts of many end users, making it a clear target for deanonymization, as we further expound upon in Section \ref{background}.

Thus, the main objective of the study herein is to deanonymize Bitcoin transacations to as great an extent as possible, as expounded upon above. The outline of this paper is as follows:
\begin{itemize}
    \item Section \ref{background}: Covers through the major technical topics of interest that are to be used in the remainder of the paper. Most of the findings presented through this section entail a literature review of well-known works in their respective fields, although occasional novel material is presented when appropriate for sake of organization. We further note that a majority of figures presented through this section entail those catered from previous research endeavors.
    \item Section \ref{methodology}: Walks through the structure of the experiments that were written and conducted. The code is freely available at: \url{https://github.com/yashpatel5400/anonychain}. We further provide the specifics of the expected structure of results from each experiment.
    \item Section \ref{results}: Presents the results of the experiments laid of in the previous section, largely in the form of graphs when appropriate and tables otherwise.
    \item Section \ref{discussion}: Discusses the main takeaways from the graphical results as they relate to the research problem posed above. Specifically, these discussions will pointedly answer the extent to which spectral clustering can be used for deanonymizing Bitcoin transactions.
    \item Section \ref{conclusion}: Provide follow-up research ideas that could be pursued should readers feel inspired and interested in furthering the research conducted herein.
\end{itemize}

\section{Background}\label{background}
To follow is the necessary technical background for the paper, of which a majority concentrates on material from graph theory. We begin by presenting the current state-of-the-art in this deanonymization space and subsequently discuss all relevant technical concepts as they relate to this study. 

\subsection{Notation}
Prior to delving into the material itself, we present a comprehensive table of the notation for the background sections below in addition to the remainder of the paper. Note that the concepts presented in Table \ref{tab:notation} will be clarified and more greatly discussed in the corresponding background section. Here we assume the standard notation of $G = (V,E)$ to be a graph with vertices $V$ and edges $E$. Such a graph can be assumed to be simple and undirected for the purposes of this paper. The specific graph of relevance will be more thoroughly explained in the methodology, from which the relevance of such assumptions will become clear.

We additionally assume the standard notation of an upper-case letter (i.e. $M$) for a matrix and lower-case (i.e. $v$) for a vector and similarly $\lambda_{i}$ to be the $i^{\text{th}}$ eigenvalue as chosen when the eigenvalues are sorted from lowest to highest (in value, not magnitude).

\begin{table}[]
\centering
\caption{Notation as used in this paper}
\label{tab:notation}
\begin{tabular}{cc}
\textbf{Symbol} & \textbf{Interpretation} \\ \hline
$G$ & Graph \\
$V$ & Vertices of a graph \\
$E$ & Edges of a graph \\
\end{tabular}
\end{table}

\subsection{Deanonymization Techniques}
Currently, two main studies have come to define the space of deanonymization, whose results and limitations we discuss herein. The objective of both was exactly that we have for this study, presented as ``our goal is not to generally deanonymize all Bitcoin users - as the abstract protocol design itself dictates that this should be impossible - but rather to identify certain idioms of use present in concrete Bitcoin network implementations that erode the anonymity of the users who engage in them" \cite{fistful}. In other words, there is no way to go from the raw BTC address information to a real-world identity. However, as with any practical system, there are often leaks in information due to sheer negligence by the end user. This parallels how, despite there being many supposed ``hacks" of Yahoo mail, this has little to do with the encryption of their internal systems, but rather due to the laziness of end users in procuring difficult-to-guess passwords.

In a similar fashion, end users often reveal information intended to be kept private in public channels. For example, people may post their wallet addresses on online forums, from which a link between the address and a real-world entity can be established. By exploiting side-channel leaked information, we can gather a base set of truth regarding who owns a particular wallet, i.e. catering a list saying that address 1Mz7153HMuxXTuR2R1t78mGSdzaAtNbBWX is owned by John Doe and so on. This list, however, will be a very tiny subset of the addresses produced on the BTC network. It is, therefore, the objective herein to expand from this tiny seed of base information to determine the identities of as many other wallets as possible. While seemingly of little use, the purpose of this becomes clear in remembering that many people use multiple BTC wallets. In other words, John Doe, from the previous example, may own three other wallets other than 1Mz7153HMuxXTuR2R1t78mGSdzaAtNbBWX. Having tainted his privacy with this public information leak, it is our objective, therefore, to associate these other three wallet IDs with his name.

Of course, while gathering a comprehensive ground truth is critical for deanonymizing as great a proportion of the network as possible, we similarly consider it successful to cluster a set of wallets as being owned by a single end user. When any one of these wallets has a public side-channel leak in the future, therefore, this information can be retroactively applied onto the clustering, from which a series of other wallets too become revealed. Thus, the act of grouping wallets as being owned by a single entity is effectively the goal of this study, though we use the procured ground truth as a method of verifying accuracy. Once again, the specifics of this procedure will be elaborated in Section \ref{methodology}.

An example as told by them is as follows: ``This clustering allows us to amplify the results of our re-identification attack: if we labeled one public key as belonging to Mt. Gox, we can now transitively taint the entire cluster containing this public key as belonging to Mt. Gox as well" \cite{fistful}. For context, Mt. Gox served a very similar role to that which Coinbase currently fills. 

Returning to the study at hand, it should therefore be clear why they ``consider de-anonymization of Bitcoin addresses as a clustering problem...Bitcoin address clustering is fairly different from classical clustering problems as there is no direct information about the objects' (addresses) such as coordinates or distances. The other peculiarity of the problem is the vastness of the Bitcoin blockchain, which requires designing computationally efficient algorithms for its' clustering" \cite{automatic}. In other words, there are two problems at hand that require solving. The first is determining and procuring a weighted graph with the property that if the edge connecting two vertices $v_{A},v_{B}$ which we denote as $w_{AB}$ satisfies: $w_{AB} > w_{BC}$, there is a higher likelihood that the wallets associated with vertices $A,B$ indeed correspond to the same real-world entity as compared to the likelihood $B,C$ are the same. The second is that of developing or discerning which clustering algorithms that are capable at running at this scale of magnitude with reasonable accuracy and time required.

We begin by discussing the first issue at hand. While it may seem natural to simply make use of the transaction ($tx$) history, seeing as there is a clear canonical mapping from this transaction to a weighted graph, it becomes clear that two accounts having many transactions between them has no bearing on being the same entity. After all, if account $A$ has many transactions with account $B$, there is no reason to believe they correspond to the same \textit{or} different people. To rephrase, we wish to procure a graph that is built atop the raw transactions with a series of heuristics, some of which have arisen from the the paper being discussed.

Prior to discussing this heuristic, we introduce the notion of a change address. ``Change addresses are the mechanism used to give back to the input user in a transaction, as Bitcoins can be divided only by being spent" \cite{fistful}. In other words, the Bitcoin protocol prevents an account that owns 10 BTC from only sending five of its BTC to another account. Instead, the account must spend the entirety of the 10 BTC, with five going to the desired recipient and the other five to a change address owned by the original owner. Having said that, we can discuss the two heuristics used for linking addresses.

\begin{theorem}
``If two (or more) addresses are inputs to the same transaction, they are controlled by the same user; i.e., for any transaction $t$, all $pk\in\text{inputs}(t)$  are controlled by the same user" \cite{fistful}.
\end{theorem}

The main idea behind this is that all the inputs of a given transaction must sign off the transaction with their private key. We assume that end users are not sharing their private keys, for revealing such information essentially gives the receiver full authority of the associated account, in terms of being able to send any BTC held in the account. It, therefore, seems appropriate to assume that end users are not sharing this information. Thus, as \textit{all} input accounts must sign off the transactions, this means all the private keys must be at present simultaneously to validate the transaction, which can only happen if a single real-world entity knows the private keys of all the associated account. This heuristic is likely the most trustworthy of the ones considered, meaning its percent of false positives is extremely low. 

While this is the case, this heuristic is not remotely comprehensive. That is to say, there are \textit{many} more wallets that are owned by the same entity as compared to those that appear together as inputs in a single transaction. Thus, it is of interest to present other heuristics that may have a lower robustness but still contains some information regarding the likelihood of corresponding to the same real-world entity. The second heuristic presented from the paper was as follows:

\begin{theorem}
``The  one-time  change  address  is  controlled  by the same user as the input addresses; i.e., for any transaction $t$,the controller of $\text{inputs}(t)$ also controls the one-time change address $pk \text{outputs}(t)$ (if such an address exists)" \cite{fistful}.
\end{theorem}

In other words, we expect that change addresses, when identifiable as such, are controlled by the originator of the transaction. To identify change addresses, the following intuition was followed: ``Working off the assumption that a change address has only one input (again,  as it is potentially unknown to its owner and is not re-used by the client), we first looked at the outputs of every trans-action. If only one of the outputs met this pattern, then we identified that output as the change address. If, however, multiple outputs had only one input and thus the change address was ambiguous, we did not label any change address for that transaction" \cite{fistful}.

Thus, this paper procured a clustering by directly applying the two heuristics on the raw transaction graph. The paper, however, faced many issues issues related to directly clustering the graph as defined by these two heuristics. The primary of which was ``falsely linking even a small number of change addresses might collapse the entire graph into large `super-clusters' that are not actually controlled by a single user" \cite{fistful}. We, thus, wish to cater heuristics that tend towards favoring false negatives as compared to false positives with the idea that the latter would result in clustering with little sematic value. In fact, the original iteration of their clustering ``ended up with a giant super-cluster containing the public keys of Mt. Gox, Instawallet, BitPay, and Silk Road, among others" \cite{fistful}. It was only after refining the second heuristic that this issue was resolved and there were distinct clusters formed. The result from this investigation is as displayed in Figure \ref{fig:fistful}. 

\begin{figure}
    \label{fig:fistful}
    \centering
    \includegraphics[width=0.4\textwidth]{fistful.png}
    \caption{The deanonymization results from `A Fistful of Bitcoins." This clearly demonstrates that, using careful catered heuristics and large-scale graph clustering, certain portions of the network can be identified with high levels of confidence. The goal of this investigation, therefore, is to procure a graph similar in nature with an even greater number of addresses identified \cite{fistful}.}
\end{figure}

As discussed in the paper itself, there are a few limitations of this investigation, some of which we seek to rectify herein. One such limitation was described as ``given that our new clustering heuristic is not fully robust in the face of changing behavior - how this gap [between actual and potential anonymity] will evolve over time, and what users can do to achieve stronger anonymity guarantee" \cite{fistful}. One key limitation, therefore, is how these two heuristics alone have assumptions that fail to hold under some of the new anonymity services that have emerged atop the BTC network. One such service is called BTC mixing.

These mixing services, also referred to BTC tumblers, effectively mix up the input addresses to a new set of output wallets. In other words, mixing services act as trusted third parties, whereby participating members can transfer their funds to this trusted service wallet, which then distributes the money amongst three new wallets, which then randomly get assigned to the participating members. Of course, the value associated with the accounts involved in the mixing must be sending identical amounts, else the owner of the accounts would be traceable through the mixing service. Assuming no collusion between the owner of the mixing service and any interested third parties, identities cannot be linked between the input and output addresses. The heuristics catered and discussed above, therefore, are unable to link addresses that have passed through such mixing services, since no two such addresses would appear in any connected transaction. This follows as it was the end user's intention in using the mixing service to disassociate the output from the input address, making it highly unlike the two would ever appear concurrently in the future.

While this is the case, assuming the end user has some static characteristics, such as making transactions at only certain times of day or to only particular accounts, the original and mixed addresses should still be linkable, marking a clear deficiency in the heuristics catered above. We, therefore, seek to integrate a broader scope of heuristics to produce potential links between accounts. Of course, with this rising set of heuristics arises the issue brought up within the Fistful of Bitcoins paper, namely that of procuring a series of false positive that ultimately collapse the overall graph into a small number of superclusters. We delve further in depth as to particular methods employed to avoid said issue, but the main workaround deals with another shortcoming of the approach employed by their clustering effort.

In particular, the heuristics they catered were rigidly employed, meaning their involvement in clustering was completely binary. That is to say, if heuristic 1 was satisfied for a pair of vertices, the two were clumped together as being the same entity immediately and similarly for heuristic 2. This rigidness was the source of the collapse and need to make refinements on the second heuristic. We wish to produce a means by which the heuristics can be procured and make the system flexible to the extent that it can automatically determine how effective the different heuristics are at linking two real-world entities and assigning appropriate weights to such heuristics. That is to say, rather than having a binary means of clustering addresses, we wish to expand to a continuous space. 

The second main investigation in the realm of deanonymization, in fact, attempts to do work along these lines. It specifically produces a probabilistic model that attempts to describe the probability two vertices in their graph correspond to the same real-world entity, where the graph contains data both from the on-chain heuristics and off-chain information. Examples of the latter are metadata associated with accounts of a particular entity, that may be used solely \textit{for} the purpose of identification, making it key in procuring a ground truth set. For example, ``Satoshi Bones Casino uses 1change and 1bones prefixes and BTC-E exchange uses 1eEUR and 1eUSD prefixes" \cite{automatic}. With the addition of this additional metadata ``heuristic," however, came additional noise in the sense that there are plenty examples thereof that do not indicate the end-user is in fact the same. As discussed in their paper, ``mostly informationless suffixes  (for example, .com, .co, @gmail)" were ignored \cite{automatic}. 

Towards that end, unlike the Fistful paper, the authors sought to procure a probabilistic model that effectively weights the different heuristics based on their levels of confidence therein. In their words, their ``proposed  model  is  not  intended  to  capture the probabilistic structure of the real world, but more to give an  approach  for  systematical  study  of  confidence  trade-offs between different sources of information" \cite{automatic}. This is similar in nature to the approach to be employed herein. However, their weights were manually tuned by the researchers such that the eventual collapses aligned with expected ground truths. Our investigation seeks to perform such tuning automatically, in a manner that parallels modern machine learning algorithms, specifically neural networks. In addition, we consider several additional heuristics of the off-chain nature described in their paper.

All in all, the current degree to which deanonymization has been accomplished demonstrates it is in fact a viable route of investigation. Yet, their limitations point to a gap in the heuristics that are being employed the the rigidity with which they are being used in doing clustering, which we seek to alleviate herein.

\subsection{Data Procurement}
As discussed in the previous section, it follows that two pieces of data need be procured for this investigation, the first being the ground truth against which perfomance of the deanonymization can be checked and the second the heuristics graph on which the clustering will be performed. For the purposes of this investigation, the gap laid by the former was filled with the ground truth gratefully provided by the authors of the Fistful of Bitcoins paper. As for the latter, however, a further investigation was conducted, from which a tool dubbed ``Blocksci" was produced. Blocksci was born out of a need to begin performing analysis on the public blockchain data. In their words, ``Bitcoin's blockchain alone is 140 GB as of August 2017, and growing quickly. This data holds the key to measuring the privacy of cryptocurrencies in practice...Blocksci is 15x-600x faster than existing tools [and] comes bundled with analytic modules" \cite{blocksci}. In particular, Blocksci fills the void of being able to iterate through the entire blockchain of transactions, allowing for heuristics to similarly be catered in an effectively online setting. That is to say, unlike heuristics procured in the previous investigations, a heuristic that involves traversing the blockchain could be done in a reasonable timeframe by employing Blocksci. Thus, one of the main bottlenecks in procuring more heuristics was eliminated as a result of the development of Blocksci.

While specific details of its internal implementation are not greatly of relevance to this investigation, we present some to make clear the process by which heuristics are produced. This is more clearly elaborated upon in Figure \ref{fig:blocksci}. Specifically, ``BlockSci's design starts with the observation that blockchains are append-only databases; further, the snapshots used for research are static. Thus, the ACID properties of transactional databases are unnecessary" \cite{blocksci}. In other words, by eliminating the unnecessary portions of database API, they streamlined the interface to end users. 

\begin{figure}
    \label{fig:blocksci}
    \centering
    \includegraphics[width=0.4\textwidth]{blocksci.png}
    \caption{The main points of relevance in this figure are the parser and analysis library. As a brief summary, the P2P node refers to the BTC node running on the client, which is constantly getting updates as they are written to blockchain by miners. Thus, the raw blockchain data sitting on the client computer will be parsed and streamlined to produce the optimized iterable data structure, which gets updated as new information arrives from the P2P node. With this iterable struture, an analysis library can be layered atop, to perform work many fold, one example of which is procuring heuristics and performing clustering \cite{blocksci}.}
\end{figure}

In a similar vein, ``The on-disk format of blockchains is highly inefficient for our purposes. It is optimized for a different set of goals such as validating transactions and ensuring immutability...whereas we aim for a single representation of the data that can it in memory" \cite{blocksci}. In other words, the parser for Blocksci implemented many optimizations such that data structures were not as bloated as they are when stored on-disk, as validation was no longer of primary concerning. Using such streamlining and properties of caching, iteration through the blockchain data was, therefore, significantly optimized.

Thus, the main takeway herein was the presence of this additional tool of Blocksci to produce heuristics atop the transactions graph.

\subsection{Overview}
Having established the current state of the art in the deanonymization space, we now turn to presenting the necessary technical background. We begin with an intuition of the layout of the investigation, from which all the pieces to be presented become apparent. Specifically, using the capabilities of Blocksci as elaborated upon above, we procure a set of eight heuristics, about which there are varying degrees of confidence. For sake of completeness, the heuristics are laid out here, as they are presented in \cite{heuristics}:

\begin{itemize}
    \item Sharing inputs in a single transaction
    \item ``If input addresses appear as an output address, the client might have reused addresses for change."
    \item ``If all inputs are of one address type (e.g., P2PKH or P2SH), it is likely that the change output has the same type"
    \item ``Most clients will generate a fresh address for the change. If an output is the first to send value to an address, it is potentially the change."
    \item ``Bitcoin Core sets the locktime to the current block height to prevent fee sniping. If all outpus have been spent, and there is only one output that has been spent in a transaction that matches this transaction’s locktime behavior, it is the change."
    \item ``If there exists an output that is smaller than any of the inputs it is likely the change. If a change output was larger than the smallest input, then the coin selection algorithm wouldn’t need to add the input in the first place."
    \item ``If tx is a peeling chain, returns the smaller output."
    \item ``Detects possible change outputs by checking for output values that are multiples of 10\^digits."
\end{itemize}

With these heuristics, unlike the previous investigations, we do not directly manipulate the original Bitcoin wallet graph. Instead, we procure a \textit{separate} graph, in which nodes correspond to wallet addresses and the edges represent the presence of a heuristic. If multiple heuristics are said to be true between two nodes, the edge connecting said nodes will be the sum of the weights associated with the corresponding heuristics. In other words, denote the weights for these heuristics as $w_1, w_2, ..., w_8$ where $w_i$ naturally takes some weight $\in[0,1]$ corresponding to heuristic $i$. We further assume that these weights are normalized, namely such that $\sum_i w_i = 1$. As an example, if heuristics 2,3, and 4 come up as true between vertices $A,B$, the weight associated with edge $e_{AB}$ will be $w_2+w_3+w_4$.

Upon curating this heuristics graph, we seek to subsequently perform clustering. Unlike the previous investigations, however, there is no rigid binary metric by which we define whether or not two vertices correspond to the same real-world entity. We instead seek to employ standard graph clustering techniques, i.e. k-means, spectral, and DBSCAN, the specifics of which are elaborated upon subsequently in the background. This eventual clustering represents our final result, whose accuracy can be ascertained by comparing whether addresses known to be the same were in fact clustered together or not and vice versa.

\subsection{Stochastic Block Model}
With that being said, this investigation began by modelling the heuristics graph in a simplified light, on which must of the initial experimentation and testing was performed. From here, many optimizations were tested and pruned and different clustering methods evaluated for use on the final dataset. The use case of studying a model rather than the full dataset is twofold. The first is that it allows rapid testing. That is to say, by being able to control exactly the size of the test graphs, many runs could be conducted for a given set of algorithms of interest. In line with that, the second benefit is having a defined metric by which performance could be measured. While there is a subset of known data points in the entire dataset, by having access to the full truth for a simulation, specific pain points could be evaluated. For example, if it turned out a particular algorithm worked well as measured by metric A but not so by metric B but metric A was of greater relevance to the final investigative result, we could easily make use of said algorithm in the final results run. In other words, such experiments made it such that the algorithms' strengths and weaknesses could be understood in great detail, since trials could be procured that targeted different circumstances.

In line with that, we begin our discussion with an introduction to the stochastic block model, commonly shortened to SBM. The SBM is a ``model for random graphs on $n$ nodes with two equal-sized clusters, with an between-class edge probability of $q$ and a within-class edge probability of $p$" \cite{sbm}. We, instead, focus on an expanded view of the SBM. That is to say, rather than solely being between two clusters, we define the SBM to have $n$ clusters such that for any clusters $C_i, C_j$, the probability a vertex in the first will be connected to a vertex in the second is $p$ if $i=j$ and $q$ if $i\neq j$, where $p > q$. In other words, this model produces a random graph comprised of many clusters, of respective sizes $n_1,n_2,...n_k$, with vertices more likely to be connected within a given cluster as compared to other clusters. This, therefore, roughly corresponds to our idealized form of the heuristics graph, namely where the heuristics will be more likely to be present between vertices that belong to the same cluster (i.e. same real-world entity). The main leak in this simplification arises from the fact that, in the case of the SBM, a single run will either have edges or not, with probability $p$ or $q$ depending on the vertices of interest. However, in expectation, namely when average over a series of $n$ runs as $n\rightarrow\infty$, the adjacency matrix will resemble that of the heuristics graph, specifically considering the case where the weights have been normalized, creating a natural correspondence to the probabilities in this matrix:

\[
\sbox0{$\begin{matrix}p&p&p\\p&p&p\\p&p&p\end{matrix}$}
\sbox1{$\begin{matrix}q&q&q\\q&q&q\\q&q&q\end{matrix}$}
%
C=\left[
\begin{array}{c|c|c|c}
\usebox{0}&\makebox[\wd0]{\large $Q$}&\makebox[\wd0]{\large $\dots$}&\usebox{1}\\
\hline
\vphantom{\usebox{0}}\makebox[\wd0]{\large $Q$}&\makebox[\wd0]{\large $P$}&\makebox[\wd0]{\large $\dots$}&\makebox[\wd0]{\large $Q$}\\
\hline
\vphantom{\usebox{0}}\makebox[\wd0]{\large $\vdots$}&\makebox[\wd0]{\large $\dots$}&\makebox[\wd0]{\large $\ddots$}&\makebox[\wd0]{\large $\vdots$}\\
\hline
\vphantom{\usebox{0}}\makebox[\wd0]{\large $Q$}&\makebox[\wd0]{\large $\dots$}&\makebox[\wd0]{\large $\dots$}&\makebox[\wd0]{\large $P$}\\
\end{array}
\right]
\]

Where each of the block $P,Q$ entries are simply referring to $(p,q)\mathbbm{1}\mathbbm{1}^T$ respectively, as indicated in some of the matrix entries. The main difference between the two arises in the fact that there is little to suggest different clusters will have similar weights in the heuristics graph. After all, it may be completely obvious that some vertices are in the same clustering, making their associated weight high, while another set of vertices have high uncertainty, making their weight lower, though the two should both be clustered. The natural extension to the SBM would simply having a different $p_i$ for each cluster $C_i$. Yet, this complicates the model to a degree such that the clustering algorithms trials cannot be conducted with the same ease as they could be otherwise. 

As a result, we sacrifice this piece of accuracy with the assumption that algorithm performance on this special case will scale appropriately to the general case. That is to say, we assume that, if algorithm A has better performance on the SBM than B, it should similarly perform better on the generalized case with varying $p_i$, though the two very likely will have degraded performance. The intuition reasoning behind said assumption is that the main detractor of performance will be when there are $p,q$ values that are comparable. Consider the edge case where $p=q$; clearly, this case can have no algorithm that performs clustering with any better accuracy than $50\%$. Thus, the bottleneck of the performance will be the lowest $p_i$ value and similarly the highest $q_{ij}$, where $q_{ij}$ denotes the natural extension of the notion of $q$ to arbitrarily many clusters, namely the probability that a vertex from $C_i,C_j$ will be adjacent. However, we can consider a simulation in which $p,q$ are relatively near one another and evaluate the algorithms' performances on said case. Since this marks the worst/most difficult case, it follows that the best algorithm will similarly scale to be the best in the easier cases. This intuition, however, is confirmed through empirical testing in Section \ref{methodology}.

We finally present one noteworthy theorem of interest with relevance to SBMs, specifically the limitations with which they can be clustered. This makes more concrete the notion that if $p,q$ are ``sufficiently close," the graph clustering cannot be performed in any manner better than chance in the limit of large graphs (where $p=a/n,q=b/n$ for a graph with a total of $n$ vertices) \cite{sbm}:

\begin{theorem}
If $a + b > 2$ and $(a-b)^2\le2(a+b)$ then, for any fixed vertices $u$ and $v$:

$$ \mathbb{P}_n(\sigma_u = + | G, \sigma_v = +) \rightarrow \frac{1}{2} a.a.s. $$
\end{theorem}

Where a.a.s. corresponds to asymptotically almost surely convergence. The specifics of this notion of convergence are not necessarily of relevance, though we put it here for sake of completeness. Namely, this corresponds to when ``An event $E$...holds with probability $1-o(1)$, thus the probability of success goes to $1$ in the limit $n\rightarrow\infty$" \cite{terry}. In other words, this is we cannot even determine whether two random vertices in the SBM are the same label or not better than random change if $a,b$ are not sufficiently different. As demonstrate in Figure \ref{fig:region}, this corresponds to where $p,q$ are quite close to one another. Note that this region is symmetric about the line $x=y$, since the notions of $p,q$ can be flipped at any point and the complexity of clustering the model would remain the same. Since the context herein applies to a generalized form of the SBM, we provide a straightforward extension of this theorem as follows:

\begin{figure}
    \label{fig:region}
    \centering
    \includegraphics[width=0.4\textwidth]{region.png}
    \caption{This marks the region in which no algorithm can perform clustering better than by chance. If the graph procured falls within this region, seeing as the $n$ considered herein is massive, it seems entirely infeasible that any algorithm will have reasonable performance.}
\end{figure}

\begin{corollary}
Assuming $\mathcal{G}=\cup_{i}\mathcal{G_i}=\bigcup_{i}(n_i,\frac{a_i}{n_i},\frac{a_i}{n_i})$, if for any choice $i$ $a_i + b_i > 2$ and $(a_i-b_i)^2\le2(a_i+b_i)$ then the problem of clustering is not solvable as $n\rightarrow\infty$.
\end{corollary}

\begin{proof}
The proof follows as a trivial consequence of the previously relayed theorem. A clustering problem is considered solvable when ``one can a.a.s. find a [cluster] which is positively correlated with the" true cluster \cite{sbm}. As any clustering algorithm must inherently cluster all subgraphs of $\mathcal{G}$, simply consider its clustering on $\mathcal{G_i}$, for which there is no positive correlation in the limit of $n\rightarrow\infty$. The overall correlation, therefore, must similarly be non-positive, meaning this overall clustering task is unsolvable.
\end{proof}

\subsection{Clustering Algorithms}
Moving on from the SBM, we look to discussing the second topic at hand, specifically graph clustering. We present the relevant algorithms, followed by specific extensions that we seek to employ due to the massive scale of the data considered herein. This discussion of clustering is summarized in a table presented following all the sections, namely table \ref{tab:algs}. We further assume that when referring to graph clustering, we wish to separate the set $V$ corresponding to $G$ into sets $\{C_1, C_2, ..., C_n\}$, such that $\cap_{i} C_{i} = \emptyset$, where the objective is to minimize the ``distance" between vertices in a single $C_i$ and maximize that between any $C_i,C_j$ for $i\neq j$, where ``distance" often has a natural interpretation as related to edge weights in the graph. 

\subsubsection{K-means}
K-means is the canonical example of a clustering algorithm. Given its age and the continual improvements in efficiency to which it has been subjected, k-means can be run efficiently on massive datasets. Specifically, ``given an integer $k$ and a set of $n$ data points in $\mathcal{R}^d$ , the goal is to choose $k$ centers so as to minimize $\phi$, the total squared distance between each point and its closest center" \cite{k-means}. The clustering of points, therefore, is simply according to which mean it is located nearest to. In other words, two vertices whose associated ``cluster mean" is the same will be clustered into the same group. The determination of such $k$ clusters, however, is an NP-hard problem, for which a heuristic has been developed and repeatedly confirmed in practice. This employed heuristic works as follows: it ``begins with k arbitrary `centers,' typically chosen uniformly at random from the data points. Each point is then assigned to the nearest center, and each center is recomputed as the center of mass of all points assigned to it. These last two steps are repeated until the process stabilizes" \cite{k-means}. 

Thus, as with many of the other algorithms considered below in this paper, k-means requires prior knowledge of the number of clusters to be considered in the end. We take an aside to address this point. While it may seem wholly arbitrary that the number of clusters be known ahead of time, as there is little way to ascertain ahead of time how many \textit{unique} users of Bitcoin there are on the network, this serves as a means by which the granularity of results can be controlled. That is to say, given two identical networks, we could potentially extract more fine clusterings with higher levels of $k$. For example, in the edge case of $k=2$, the clustering would likely resemble something along the lines of ``Coinbase accounts" and ``Non-Coinbase accounts," namely a separation of a supercluster from the remainder of the graph. We can, therefore, imagine that $k\rightarrow\infty$, clusters associated with individual people or small organizations would form out of the massive agglomerated ``Non-Coinbase" cluster. In the limit, however, we would simply end up with singleton clusters, which would serve little purpose for identifying real-world entities. The selection of $k$, therefore, is a balancing act between getting fine grain separation of individual entities and uninformative results.

Thus, the main use case of k-means herein will be as a baseline for getting results. That is to say, we expect \textit{all} other algorithms employed to have superior performance than k-means, though they may suffer greatly in time complexity.

\subsubsection{Spectral}
Spectral clustering, as implied by its name, deals with clustering on the basis of eigenvalues and eigenvectors. Which eigenvectors to be used for such clustering is not immediately obvious. However, it turns out to be those of a matrix that has fundamental importance to its corresponding graph: the Laplacian. While the Laplacian has differing definitions by context, in the realm of graph theory, it simply refers to:

$$ L = D-W $$

Where hereto we denote $L$ as the Laplacian matrix; $D$ the degree matrix, containing the degree of vertex $i$ in position $D_{ii}$ and 0s elsewhere; and $W$ the weight matrix, containing the weight of edge $e_{ij}$ in position $D_{ij}$. Since $D,W$ are both symmetric matrices in the case of an undirected graph, as we have here, $L$ too is symmetric. Having presented the definition, we follow up with some relevant theorems regarding this Laplacian matrix \cite{spectral}:

\begin{theorem}
\begin{itemize}
    \item The smallest eigenvalue of $L$ is $0$, the corresponding eigenvector is the constant one vector $\mathbbm{1}$.
    \item $L$ has $n$ non-negative, real-valued eigenvalues $0=\lambda_1\le\lambda_1\le\dots\le\lambda_n$.
\end{itemize}
\end{theorem}

From this theorem spawn several other results which have direct consequences in the use of eigenvectors for the purposes of graph clustering. The first theorem is as follows:

\begin{theorem}
Let $G$ be an undirected graph with non-negative weights. Then the multiplicity $k$ of the eigenvalue 0 of $L$ equals the number of connected components $A_1,\dots,A_k$ in the graph. The eigenspace of eigenvalue $0$ is spanned by the indicator vectors $\mathbbm{1}_{A_1},\dots,\mathbbm{1}_{A_k}$ of those components.
\end{theorem}

While we refer to \cite{spectral} for the whole proof of this result, we present a sketch of the primary portion of this proof from which an intuitive understanding arises. Specifically, consider the idealized case of a graph $\mathcal{G}$ containing clusters such that vertices within the clusters are fully connected and those in different clusters are not adjacent. The corresponding graph Laplacian for such a $\mathcal{G}$ would be:

\[
\begin{bmatrix}
    L_{1} &  & &  \\
    & L_{2} & &  \\
    & & \ddots &  \\
    & & & L_{n}
\end{bmatrix}
\]

Where each $L_i$ is the Laplacian of the subgraph that only contains cluster $i$. From here, we notice that the eigenvectors of $L$ will simply be those that correspond to those of each of the $L_i$ with 0s filling up the entries that do not correspond to the $L_i$ matrix entries. By the preceding thereom, we know that each of these Laplacians have an eigenvector of $\mathbbm{1}$ corresponding to the eigenvalue 1, meaning that the multiplicity of the eigenvalue 1 relates to how many block entries there are in this Laplacian matrix, which precisely corresponds to the number of clusters there are. 

We now wish to understand how to go about using such eigenvectors to accomplish clusterings. Specifically, to do so, consider once again the idealized case. In creating a matrix of these $k$ eigenvectors corresponding to the clusters as columns, we are left with a matrix of the form (where we are depicting the matrix as if the clusters are all of size 2):

$$ M = [\mathbbm{1}_{A_1}^T,\mathbbm{1}_{A_2}^T,\dots,\mathbbm{1}_{A_k}^T] = 
\begin{bmatrix}
    1 & 0 & \dots & 0 \\
    1 & 0 & \dots & 0 \\
    0 & 1 & \dots & 0 \\
    0 & 1 & \dots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \dots & 1
\end{bmatrix}
$$

From here, we simply notice that there are precisely as many rows as there were originally vertices in the graph, since each row corresponds to an eigenvector, which corresponded to the Laplacian matrix. Noticing that, it becomes clear that these vertices can now be trivially clustered using the k-means algorithm with perfect accuracy. Thus, the main takeaway from this example is understanding that the eigenvectors corresponding to eigenvalues of lowest magnitude essentially help transform the vertices into a new space that much more closely correspond to their associated cluster, from which the k-means algorithm can trivially be employed to complete clustering. The eigenvectors with lowest eigenvalues are employed as they will be closest to the idealized $\mathbbm{1}_{A_i}$ vectors. In addition, the remaining eigenvalues will be well separated from this set of low-magnitude eigenvalues. As a result, this points to a natural method by which we can estimate the number of clear clusters, namely by determining how many eigenvalues are near 0 prior to there being a jump in magnitude, as portrayed in Figure \ref{fig:spectral_example}.

\begin{figure}
    \label{fig:spectral_example}
    \centering
    \includegraphics[width=0.4\textwidth]{spectral_example.png}
    \caption{The top left is a histogram of the data spread and the remainder of the picture corresponds to the eigenvalues and eigenvectors of the Laplacian, as discussed in the exposition. From this, it is clear that a well-separated dataset has a marked jump from the eigenvalues corresponding to the the clusters and the remaining ones \cite{spectral}.}
\end{figure}

This procedure is summarized in the presentation of Algorithm \ref{alg:spectral-kmeans}. Note that their conception of spectral clustering involves beginning with a similarity matrix, namely a symmetric matrix where entry $S_{ij}$ corresponds to some notion of ``similarity" between vertices $i,j$. It is from this matrix that a similarity graph is \textit{constructed} and its Laplacian determined. There exist many ways by which such a similarity graph can be constructed. As enumerated in \cite{spectral}, three such ways are as follow:

\begin{itemize}
    \item \textbf{$\epsilon$-neighborhood graph}: Connect vertices together if the similarity metric defined between the two of them exceeds some threshold parameter value of $\epsilon$. That is to say, the adjacency matrix will have edges according to $\mathbbm{1}_{e_{ij}>\epsilon}$.
    \item \textbf{$k$-nearest neighbors}: Connect a given vertex to its $k$ nearest neighbors in sense defined by the similarity metric, i.e. create an edge for the vertices corresponding to the $k$ highest similarity measures. While this is normally asymmetric, simply avoid this problem by creating an undirected edge between the two vertices. That is to say, an edge between $v_i,v_j$ will be present if \textit{either} $v_j$ is one of the $k$-nearest neighbors for $v_i$ or vice versa (or both).
    \item \textbf{Fully-connected graph}: Connect all edges, weighting the edges with some similiarity function, i.e. a Gaussian $G(v_i,v_j) = \text{exp}\left(e^{-\frac{|| v_i - v_j ||^2}{2\sigma^2}}\right)$
\end{itemize}

The procedure, therefore, thereafter is held as described above. For our particular case, we elect to use the third method of catering a similarity graph, although future elaborations on this work may expand this assumption and attempt other methods to determine their relative success. 

\begin{algorithm}
    
\caption{Constructs Laplacian from the similarity matrix}\label{alg:laplacian}
\begin{algorithmic}[1]
\Procedure{ConstructLaplacian(S,k)}{} \\
\textbf{Input:} Similarity matrix $S\in\mathbb{R}^{n\times n}$ \\
\textbf{Input:} Cluster count $k$

\State $G \gets \text{Simliarity graph for } S$
\State $W\in\mathbb{R}^{n\times n} \gets \text{Weighted adjacency matrix for } G$ \\
\State $L \gets D - W$ \\

\textbf{Output:} Laplacian matrix $L$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{K-means based spectral clustering \cite{spectral}}\label{alg:spectral-kmeans}
\begin{algorithmic}[1]
\Procedure{SpectralClustering-Kmeans(S,k)}{} \\
\textbf{Input:} Similarity matrix $S\in\mathbb{R}^{n\times n}$ \\
\textbf{Input:} Cluster count $k$

\State $L \gets ConstructLaplacian(S,k)$
\State $v_1,v_2,...,v_k \gets \text{first } k \text{ (sorted by } \lambda_i \text{) eigenvectors of } L$
\State $V\in\mathbb{R}^{n\times k} \gets [v_1^T, v_2^T,\dots,v_k^T]$
\State $(y_i)_{i=[1:k]} \gets V^T_{i,:} \text{ (i.e. row } i \text{of } V\text{)}$
\State $(C_i)_{i=[1:k]} \gets kmeans((y_i)_{i=[1:k]})$ \\
\textbf{Output:} $(C_i)_{i=[1:k]}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

In addition to this procedure, however, there is another which relies on a different principle, albeit still related to the spectrum of the Laplacian matrix. In this alternate approach, the ``eigenvector $u_2$ corresponding to $\lambda_2$ (the second-smallest eigenvalue) is computed, and the vertices of the graph are partitioned according to the values of their corresponding entries in $u_2$" \cite{bisection}. In other words, the second eigenvector is computed and used to partition the graph into two (or three) sections, specifically corresponding to the signs of the entries of the eigenvector. This process can be subsequently repeated a number of times to achieve the desired number of $k$ clusters. Further iterations of the algorithm are selected on the basis of their second eigenvalue, namely by determining that which has the lowest eigenvalue, since this effectively corresponds to how well separable the cluster is.

Returning to the previous example at hand, namely the idealized case, intuition for this algorithm becomes clear. Specifically, we have that the eigenvector of the first eigenvalue (which \textit{always} corresponds to an eigenvalue of 0) will simply be $\mathbbm{1}$, meaning there is no information contained in its entries that allow for separation. Instead using the second eigenvector, we have something of the form $[\mathbbm{0}_{A_1},\dots\mathbbm{0}_{A_{k-1}},\mathbbm{1}_{A_k},\dots\mathbbm{0}_{A_n}]$. In this case, we see that partitioning by sign would effetively separate a cluster ($A_k$ in this particular instance) from the remainder of the graph. Presumably, in the following iteration, the second eigenvalue corresponding to the subgraph of $\mathcal{G}\\A_k$ will be less than that of $A_k$, meaning it will be partitioned in the following iteration. Once again, the second lowest eigenvalue is chosen as it roughly demonstrates the corresponding cluster is the closest to the idealized separable case of all those present in the graph. We refer to this version of spectral clustering as ``hierarchical," as it involves repeated bisection and employs a top-down separation method as contrasted with the k-means spectral implementation, which clusters all vertices at once. It seems natural, therefore, that hierarchical will be more accurate but significantly slower, due to its more careful handling of separation, than its k-means counterpart, a trend we investigate more fully empirically. This procedure is fully summarized in Algorithm \ref{alg:spectral-hierarchical}. 

\begin{algorithm}
\caption{Hierarchical based spectral clustering \cite{spectral}}\label{alg:spectral-hierarchical}
\begin{algorithmic}[1]
\Procedure{SpectralClustering-Hierarchical(S,k)}{} \\
\textbf{Input:} Similarity matrix $S\in\mathbb{R}^{n\times n}$ \\
\textbf{Input:} Cluster count $k$

\State $C \gets [G]$
\For {$i=1:k-1$}
    \State $\lambda_{min} \gets None$, $v_{min} \gets None$, $j_{min} \gets None$
    
    \For {$j=1:len(C)$}
        \State $W\in\mathbb{R}^{n\times n} \gets \text{Adjacency matrix for } C[i]$
        \State $L_i \gets D_i - W_i$
        \State $\lambda_2,v_2\gets\text{2nd eigenvalue, eigenvector of } L_i$
        \If {$\lambda_2 < \lambda_min$}
            \State $\lambda_{min} = \lambda_2$
            \State $v_{min} = v_2$
            \State $j_{min} = j$
        \EndIf
        
        \State $C_{new} = [Set(),Set()]$
        \For {$k=1:len(v_{min})$}
            \If {$v_{min}[k] > 0$}
                $C_{new}[0].add(v_min[k])$
            \Else
                $C_{new}[1].add(v_min[k])$
            \EndIf
        \EndFor
        
        \State $C[j_{min}] \gets C_{new}$
    \EndFor
    
\EndFor

\textbf{Output:} $C$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Thus, having discussed spectral clustering, we briefly discuss its role in this investigation. Spectral clustering represents one of the major departures from previous studies that we wished to bring forward to this investigation, since it markedly involves attempting to perform clustering in a higher dimensional space, capable of capturing non-planar clusters. This contrasts previous studies, whose clustering, by virtue of directly working with the transaction graphs, cannot obtain any results that are non-planar. Hence, we wish to observe whether any clusters are more easily discernable under the mapping produced in spectral clustering as compared to their planar counterparts.

\subsubsection{DBSCAN}
DBSCAN is markedly different from the previous two algorithms in that it expressly seeks to provide an algorithm that does not need a number of clusters to be supplied. Much in the way spectral clustering could be used to estimate the number of clusters using a ``separation gap," DBSCAN similarly relies on two user-supplied parameters that effectively serve as the thresholds to be considered as part of the same cluster, though the manner in which these parameters serve as a threshold is slightly more involved. Prior to diving more in depth about its inner workings, DBSCAN additionally makes it an explicit goal to not necessarily cluster all vertices. That is to say, many of the vertices may end in a state of being in an ``undefined" category with respect to how it clusters. This is quite natural, in that there are often circumstances where vertices are simply shoehorned into a particular premade cluster simply because it is the best of the available options when in reality it is clear said vertex does not fit well in \textit{any} of these clusters.

Returning to the algorithm at hand, the user-supplied parameters are typically denoted as $\epsilon$ and $minPts$. From this parameter of choice, a set of ``core points" $[p_i]$ become defined, where a core point is any point $p\in G$ such that there are $\ge minPts$ other points $q$ such that the distance $d_{pq} < \epsilon$. That is to say, a core point essentially captures the intuitive notion of vertices that are ``central" in the graph, i.e. are well-connected to a significant number of other vertices in the graph. From there, we define one vertex $v_B$ being \textit{directly} reachable from another $v_B$ if the other vertex is a distance $d_{AB} < \epsilon$. We now define an arbitrary vertex $v_B$ as being reachable from another $v_A$ if there is a path of pairwise directly reachable core points that terminate in $v_B$. This is more clearly illustrated in Figure \ref{fig:dbscan}.

To finally construct the clusters, we simply go through the set of core points and find all other vertices that are reachable from these seeds. If a vertex is not reachable from any core points, it is simply remains unclustered in the final result \cite{dbscan}.

\begin{figure}
    \label{fig:dbscan}
    \centering
    \includegraphics[width=0.4\textwidth]{dbscan.png}
    \caption{Here the $minPts=4$. Red points correspond to the core points, yellow those that are reachable from core points, and blue that not reachable from any. Thus, in this case, all the vertices would be clustered together with the exception of the blue vertex \cite{wiki-dbscan}.}
\end{figure}

The main advantage of DBSCAN is that it is extremely well suited for large-scale clustering. DBSCAN additionally is capable of capturing non-flat cluster geometry, similar to spectral clustering, due to its ability to capture long-link separated clusters. Thus, it provides a means of clustering that effectively takes the superior speed of k-means and clustering flexibility of spectral into a single algorithm. Unfortunately, DBSCAN is extremely sensitive to its parameters, and its choice is oftentimes non-obvious, as it holds little semantic meaning outside of the context of the algorithm itself.

To summarize, we present these strengths and necessary parameters of the different vanilla algorithms considered in the previous sections of the background in Table \ref{tab:algs}. Note that these results were as implemented by scikit-learn, meaning there could be variations of the implementations of these algorithms that see parameter use differently.

\begin{table}[]
\centering
\caption{Summary of clustering algorithms \cite{scikit}.}
\label{tab:algs}
\begin{tabular}{llll}
\textbf{Method name} & \textbf{Parameters} & \textbf{Scalability}                                                                                     & \textbf{Use Case}                                                                                                   \\ \hline
K-Means              & number of clusters  & \begin{tabular}[c]{@{}l@{}}Very large n\_samples, medium \\ n\_clusters with Minibatch code\end{tabular} & \begin{tabular}[c]{@{}l@{}}General purpose, even cluster size, \\ flat geometry, not too many clusters\end{tabular} \\
Spectral clustering  & number of clusters  & \begin{tabular}[c]{@{}l@{}}Medium n\_samples, \\ small n\_clusters\end{tabular}                          & \begin{tabular}[c]{@{}l@{}}Few clusters, even cluster size, \\ non-flat geometry\end{tabular}                       \\
DBSCAN               & neighborhood size   & \begin{tabular}[c]{@{}l@{}}Very large n\_samples, \\ medium n\_clusters\end{tabular}                     & \begin{tabular}[c]{@{}l@{}}Non-flat geometry, uneven \\ cluster sizes\end{tabular}                             
\end{tabular}
\end{table}

\subsection{Clustering Evaluation}
Having discussed these clustering methods, it seems natural that there be a metric by which they can be compared. This seemingly trivial property, however, is not completely obvious in its definition as may seemingly be the case upon first glance. Specifically, we assume for the sake of this discussion that there is some known ground truth clustering of the vertices. Unlike the DBSCAN assumption, we further assume that \textit{all} vertices are associated with one (and exactly one) cluster of the ground truth. 

Having said that, there emerge four natural metrics of evaluating clustering, per \cite{evaluation}. The notation employed by this resource, and that which we extend for the remainder of this section, is `` $\Omega = \{ \omega_1, \omega_2, \ldots, \omega_K \}$is the set of clusters and $\mathbb{C} = \{ c_1,c_2,\ldots,c_J \}$  is the set of classes" \cite{evaluation}. In other words, $\Omega$ corresponds to the guessed clusters while $C$ is the set of ground truth clusters, which are referred to as ``classes." 

The first metric is referred to as \textit{purity} and is defined as:

$$ \text{purity}( \Omega,\mathbb{C} ) = \frac{1}{N} \sum_k \max_j \vert\omega_k \cap c_j\vert $$

Roughly, the intuition behind purity is that a clustering with high purity has clusters such that the elements that are clustered together are in truth originating from the same class. In other words, an impure clustering would have clusters with vertices of differing classes, whereas a perfect clustering would have a purity of 1.0. It follows that as $k\rightarrow\infty$, the purity will tend towards 1.0 and will be exactly so once $k=n$. In other words, once there are sufficiently many clusters that each vertex gets associated to its own cluster, the purity there and for any higher values of $k$ will be 1.0. Thus, this metric, while useful in ascertaining the overlap with the true classification, suffers from generally improved ``performance" as the granularity increases. 

Another metric is the ``normalized mutual information or NMI:

$$ \text{NMI}(\Omega , \mathbb{C}) = \frac{ I(\Omega ; \mathbb{C}) } { [H(\Omega)+ H(\mathbb{C} )]/2 } $$

[where] $I$ is mutual information

\begin{align}
I( \Omega ; \mathbb{C} ) &= \sum_k \sum_j P(\omega_k \cap c_j) \log \frac{P(\omega_k \cap c_j)}{P(\omega_k)P(c_j)} \\
&= \sum_k \sum_j \frac{\vert\omega_k \cap c_j\vert}{N} \log \frac{N\vert\omega_k \cap c_j\vert}{\vert\omega_k\vert\vert c_j\vert}
\end{align}

where $P(\omega_k)$, $P(c_j)$, and $P(\omega_k \cap c_j)$ are the probabilities of a document being in cluster $\omega_k$, class $c_j$, and in the intersection of $\omega_k$ and $c_j$, respectively... $H$ is entropy as defined...:

\begin{align}
H(\Omega) &= -\sum_k P(\omega_k) \log P(\omega_k)
&= -\sum_k \frac{\vert\omega_k\vert}{N} \log \frac{\vert\omega_k\vert}{N}
\end{align}

where...the second equation is based on maximum likelihood estimates of the probabilities" \cite{evaluation}. Having presented this metric, it serves well to parse the definition. The intuition behind this metric becomes more evident in understanding the notions of mutual information and entropy separately. In fact, the latter is simply a special case of the former, where $H(\Omega) = I(\Omega,\Omega)$. Returning to mutual information, this measures the gain in information from being presented a new piece of information. Intuitively, it is the reduction in uncertainty of the state of some random variable $B$ by learning that for some r.v. $A$. In this particular case, it is measuring the extent to which knowing our guessed clusters helps in knowing the class. Clearly, in the case of a perfect reconstruction of the true clustering, knowing our guess will greatly increase our knowledge of the class, whereas an ill-formed guess will give no bearing as to what the true class is.

It, once again, follows that the mutual information increases to its max value as $k\rightarrow\infty$, namely once there are sufficiently many clusters to assign one per vertex. Taking an aside, as previously discussed, if we are able to recreate the ground truth clusters perfectly, the maximum value of MI is achieved. However, by looking at the definition, it becomes clear that if these clusters are further subdivided, the resulting MI remains the same. That is to say, if we have one additional cluster than the number present in the ground truth but the clusters $C_1,\dots,C_{n-1}$ correspond exactly to those in the truth and $C_n,C_{n+1}$ correspond to the underlying $n$th true cluster, the MI will still be maximized. In particular, this means in the case of each vertex being assigned to its own cluster, as this is a refined clustering of the ground truth, the max MI will \textit{also} be attained in this case. Thus, to combat this issue, we normalize by the entropy, which ``tends to increase with the number of clusters" \cite{evaluation}. Thus, by combining these two into a single NMI metric, we are able to effectively measure that the correct number of clusters were extracted in addition to the contents of the clusters being correctly formed.

The final two metrics are directly related to one another, with thie third simply being a special case of the fourth. This metric, known as the Rand index, is, in fact, the simplest to present and interpret. We simply consider all $n(n-1)/2$ pairs of vertices and calculate:

$$ RI = \frac{TP + TN}{TP + TN + FP + FN} $$

Where TN, TP, FP, and FN are respectively the true negative, true positive, false positive, and false negative classifications that were made. Thus, if a pair of vertices were clustered togethered when they were in the same class in reality, this would be a TP, whereas, if they were not in reality, this would be an FP. The TN and FN are similarly defined if a pair of vertices are clustered separately. Thus, this metric is simply the ``total correct" of the total pairwise predictions being made. Clearly, the main limitation in this formulation of a metric is that it weighs FN and FP errors equally. While this may occasionally be the case, it is not so for us, as discussed by the previous works in deanonymization, wherein even a small number of false positives cause the collapse of the entire graph into a single supercluster. It would, therefore, seem quite natural to assume FN should be penalized less harshly as contrasted to FP in our case.

Extending to include this simply adjustment is quite straightforward, using the well-known F measure from statistics:

\begin{align}
P &= \frac{TP}{TP+FP} \\
R &= \frac{TP}{TP+FN} \\
F_{\beta} &= \frac{(\beta^2+1)PR}{\beta^2 P+R}
\end{align}

Where false positives would be weighted more heavily for small values of $\beta$, namely $\beta<1$ \cite{evaluation}. Having discussed these metrics by which clustering can be evaluated, we turn to specific techniques employed for the study herein due to the massive scale of the data at hand.

\subsection{Matrix Estimation}
As mentioned previously, the scale of the BTC transaction graph is massive, making the application of vanilla clustering algorithms practically impossible should we wish to achieve any sensible predictions. Note that, while the heuristics graph contains information of a different nature than the transaction graph, it is roughly the same order of magnitude of scale. While each heuristic likely only applies to a select subset of the entire of graph vertices, the introduction of all the heuristics, each of which less rigid and certain than those originally conceived, made it such that a comparable number of edges exist in the heuristics graph. To get a sense of scale, the graph to be clustered had roughly 200 million vertices and 10 billion edges, roughly corresponding to 180 GB of disk space. Thus, in addition to requiring far too much time to execute, handling the data all at once would require all this be stored in memory, meaning we had to determine approaches that either handled execution of clustering in some online setting or an approximated form. We discuss the former in this and the following section and the latter in the final two sections of the Background.

We now briefly discuss the basic concepts behind matrix estimation. Though the main concepts presented in this section do no directly come up in the investigation herein, they form the basis for some the online clustering algorithms presented in the following section, making this information still relevant to understand. Clearly, ``the optimal low rank approximation for any matrix is obtained by its truncated Singular Value Decompositions (SVD)," as this is precisely the result obtained in solving the corresponding Rayleigh Quotient problem \cite{sketch}. It, therefore, may seem unnecessary for any other estimation methods to be developed, seeing as the \textit{optimal} estimation for any arbitrary lower dimension is already known. However, the need becomes apparent in observing ``Data matrices...[that are] extremely large and distributed across many machines...render standard SVD algorithms infeasible" \cite{sketch}.

Towards this end, our particular application has the former property, wherein the data is too massive to perform an SVD. Matrix sketching is one such alternate approximation method. ``There are three main matrix sketching approaches...The first generates a sparser version of the matrix...The second approach is to randomly combine matrix rows.... The third sketching approach is to find a small subset of matrix rows (or columns) that approximate the entire matrix" \cite{sketch}. In other words, all methods of matrix sketching seek to construct some smaller matrix by directly manipulating the original matrix rows. Unlike the procedure of extracting vectors from SVD, there are no additional computations required herein with the exception of manipulations performed on the initial original matrix rows. The paper additionally ``proposes a fourth approach. It draws on the similarity between the matrix sketching problem and the item frequency estimation problem" \cite{sketch}.

They specifically provide two methods that fall into this classification of matrix sketching, though the specifics of their inner workings are not of relevance to this study. The idea revolves around extending a clever approximation algorithm that solves the seemingly unrelated problem of estimating the count of $m$ distinct items as they appear in a data stream while using less memory than that required to have a counter per item. Specifically, their matrix sketching technique views the rows of a matrix as ``items." To gain an intuitive understanding, consider the simplified case where the matrix \textit{only} contains $e_i$ vectors, i.e. the standard $\mathbb{R}^n$ basis vectors. In this case, by applying this ``frequency counter" idea, we can construct a matrix of the basis vectors that appeared with greatest frequency, which effectively captures an approximation of the original matrix.

The main issue in this arises in considering a matrix with rows of differing magnitudes and multiple non-zero entries. For example, the frequency counter when applied to a matrix whose first row is $\mathbbm{1}_n$ and second is $2\mathbbm{1}_n$ would view the two as completely distinct, when they point in the same direction in reality and, therefore, should add to some common counter for a third average row vector. Towards that end, the authors therein considered a refinement of the frequency counter they dubbed ``frequent-directions," which seeks to solve this very issue. We defer to their paper for full presentation of detail, as this understanding suffices for our purposes.

\subsection{Online Clustering}
Having presented this brief background on matrix approximation, we turn to the first method considered in performing clustering on the massive heuristic graph, namely that of performing online graph clustering. Of course, we use the term ``online" in the traditional statistical sense, wherein the analysis is to be performed as data arrives rather than all at once, in a single process after the data has been fully collected. This has a natural correspondence to our setting, assuming Blocksci is used to procure the data incrementally. In viewing our problem as an online clustering problem, we will have data that arrives that adds new data points, i.e. when we encounter a node who is exchanging BTC for the first time, and changing of weights of previously present nodes, i.e. when a heuristic is predicted as being true for a previously seen node that was previously marked false or vice versa. Thus, the online scheme employed \textit{must} be capable of adapting to both the addition and changing of previously seen data points, though handling of deletion is unnecessary, as a vertex will permanently remain in the graph after first being encountered.

We additionally discuss why some of the algorithms presented and discussed here could not be applied in their originally presented form to the case at hand. All the algorithms presented are online versions of spectral clustering. We discuss two variations thereof, namely the streaming setting and evolutionary setting. Much work has been done in the realm of streaming clustering, as fully discussed in \cite{streaming} and \cite{eigen-update}. Having said that, the reason these algorithms fail to meet the demands of the task at hand is because none of them support the change of weights required for our purposes. That is to say, while they perform clustering on data streams, they assume such streams are simply the addition of new data points rather than relating to those that already exist.

Towards that end, an alternate dubbed ``evolutionary clustering" has arisen that seeks to fill this gap where weights can be newly created or changed from preexisting values. The main work in this space is \cite{incremental}, which states ``our approach is the first work accomplishing the task of incremental spectral clustering that can handle not only insertion/removal of data points but also similarity changes...Evolutionary clustering simultaneously optimizes two potentially conflicting criteria, i.e., the clustering should fit the current data as much as possible, while should not deviate dramatically from the historic context." This, therefore, fits our setting quite well, with one exception, namely that of temporally smoothness. While it is generally the case that temporal smoothness is a desirable property, especially where the data itself is modelling a process that itself is evolving in nature, this context does not fit that description. Specifically, the act of two vertices corresponding to the same person is not a continuous process; it is simply a binary fact about which we have information arriving incrementally. In other words, it is very possible, and almost certainly the case, that up until some transaction $t$, vertices $v_A,v_B$ will be thought to correspond to two different clusters/people but transaction $t$ shows $v_A,v_B$ both as inputs, meaning (by Heuristic 1 previously discussed) we would want to \textit{always} consider these two as being in the same cluster thereafter. In other words, there was a distinct jump from \textbf{not} being in the same cluster to being clustered together at time \textit{t}. To avoid this issue, however, the evolutionary clustering techniques employed have a tunable weight parameter that determines the extent to which temporal smoothness is desired, for which we would simply set an extremely low value.

The algorithm, therefore, relies on incremental updating to both the eigenvalues and eigenvectors of the Laplacian as would naturally be the case for updating spectral clustering. We specifically refer to the results and algorithms catered in \cite{incremental}, which found:

\begin{align}
    \Delta v_{ij} = (K^T_{\mathcal{N}_{ij}} K_{\mathcal{N}_{ij}})^{-1} K_{\mathcal{N}_{ij}} \textbf{h}\label{eq:delta-v}
\end{align}

Where (for an eigenvector $\textbf{v}$ and some fixed threshold $\tau$):

\begin{align}
    K = L - \lambda D \\
    \textbf{h} = (\Delta\lambda D + \lambda \Delta D - \Delta L)\textbf{v} \\
    \mathcal{N}_{ij} = \{k|w_{ik} > \tau \text{ or } w_{jk} > \tau\}
\end{align}

\begin{align}
    \Delta\lambda = \Delta w_{ij}\frac{a + b}{1 + c + d}\label{eq:delta-lambda}
\end{align}

Where ($v_i$ denoting the $i$th component of $v$ and $d_k$ is degree of vertex $k$):

\begin{align}
    a = (v_i - v_j)^2 - \lambda(v_i^2 + v_j^2) \\
    b = (v_i - v_j)(\Delta v_i - \Delta v_j) - \lambda(v_i\Delta v_i + v_j\Delta v_j) \\
    c = \Delta w_{ij}(v_i^2 + v_j^2) \\
    d = \sum_{k\in\mathcal{N}_{ij}} q_k d_k\Delta q_k
\end{align}

Having defined these update functions, we provide the final algorithm that was developed, namely in \ref{alg:spectral-iterative}.

\begin{algorithm}
\caption{Refinement of $\Delta\lambda$ and $\Delta v$ \cite{incremental}}\label{alg:refinement}
\begin{algorithmic}[1]
\Procedure{Refine($\lambda, v, D, \Delta D, L, \Delta L, \Delta W$)}{} \\
\textbf{Input:} Current eigenvalue $\lambda$ \\
\textbf{Input:} Current associated eigenvector $v$ \\
\textbf{Input:} Previous time-step degree matrix $D$ \\
\textbf{Input:} Time-step degree matrix update $\Delta D$ \\
\textbf{Input:} Previous time-step Laplacian matrix $L$ \\
\textbf{Input:} Time-step Laplacian matrix update $\Delta L$ \\
\textbf{Input:} Time-step weight matrix update $\Delta W$

\State $\Delta v \gets 0$
\While $\Delta v, \Delta\lambda \text{ not stabilized}$
    \State $\Delta\lambda \gets$ \ref{eq:delta-lambda} using current $\Delta v$ 
    \State $\Delta v \gets$ \ref{eq:delta-v} using current $\Delta \lambda$ 
\EndWhile

\textbf{Output:} $\Delta v, \Delta\lambda$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Iterative (online) spectral clustering \cite{incremental}}\label{alg:spectral-iterative}
\begin{algorithmic}[1]
\Procedure{SpectralClustering-Iterative(D,k)}{} \\
\textbf{Input:} Data stream (individual vertices with heuristics) $D$ \\
\textbf{Input:} Cluster count $k$

\State $t \gets\text{ time enough data points to define } W, L, D$
\State $G \gets \emptyset$
\For {$i=1:t$}
    \State $G \gets S[i] \text{update vertices/edges}$
\EndFor

\State $W\gets W(G), L\gets L(G), D\gets D(G)$
\State $[v_i], [\lambda_i] \gets eigendecomp(L)$
\While {$S[j] \text{not null}$}
    \State $i \gets \{v_i|S[j]_i > 0\}$
    \State $\Delta W\gets W - S[j] e_i$
    \State $\Delta D\gets D - S[j] e_i$
    \State $L\gets \Delta W + \Delta D$
    \State $\Delta v_i, \Delta \lambda_i \gets Refine(\lambda_i, v_i, D, \Delta D, L, \Delta L, \Delta W)$
    \State $Update(W,D,L,v_i,\lambda_i)$
\EndWhile

\State $V\in\mathbb{R}^{n\times k} \gets [v_1^T, v_2^T,\dots,v_k^T]$
\State $(y_i)_{i=[1:k]} \gets V^T_{i,:} \text{ (i.e. row } i \text{of } V\text{)}$
\State $(C_i)_{i=[1:k]} \gets kmeans((y_i)_{i=[1:k]})$ \\

\textbf{Output:} $C$
\EndProcedure
\end{algorithmic}
\end{algorithm}

Thus, in this way, an online implementation of spectral clustering was developed and applied to the context of BTC wallet deanonymization.

\subsection{Graph Sparsification}
The second method to employ for our purposes are directly approximating the graph. Namely, rather than performing the clustering on the original heuristic graph $\mathcal{G}$, we could procure some related graph $\mathcal{G}'$, such that, after clustering is performed on this related graph, the clustering could naturally be extended to the original $\mathcal{G}$. Towards that end, we look to both graph sparsification and coarsening; both seek to produce a $\mathcal{G}'$ that is both simpler than and similar to the original $\mathcal{G}$, in some sense defined by the particular method employed. Sparsification seeks to produce a simpler graph by removing edges of the original graph, whereas coarsening seeks to do so by removing vertices.

As we discussed in the previous sections, namely in that regarding spectral clustering, information of the separation of clusters is inherently present in the spectrum of the corresponding Laplacian matrix. It, therefore, seems reasonable to perform graph approximation with the express goal of retaining a Laplacian matrix as close to the original as possible. What is meant by ``close" is once again ambiguous by the various matrix metrics that are typically employed. Towards that end, we use what has become the standard, namely the notion two matrices being $\sigma-$spectrally similar. Prior to defining this notion, we introduce some associated notation, namely:

$$ A \preceq B \implies x^T Ax \le x^T Bx \forall x $$

Having introduced that, ``We say $A$ and $B$ are $\sigma-$spectrally similar if:

$$ B / \sigma \preceq A \preceq \sigma\cdot B $$" \cite{spectral-sparse}.

This notion of matrices being $\sigma$-spectrally similar, while well-defined, is not immediately applicable to graphs. We, therefore, discuss an equivalent formulation thereof that directly resolves this opaque application. Specifically, the ``Courant-Fisher Theorem tells us that:

$$ \lambda_i(A) = \max_{S:\text{dim}(S)=i} \min_{x\in S} \frac{x^T Ax}{x^T x} $$

Thus, if $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $A$ and $\widetilde{λ_1},\dots,\widetilde{\lambda_n}$ are the eigenvalues of $B$, then for all $i$, $\lambda_i / \sigma \le \widetilde{\lambda_i} \le \sigma\cdot\lambda_i$. Using this notation, we can now write inequality as:

$$ L_{\widetilde{G}} / \sigma \preceq L_{G} \preceq \sigma \cdot L_{\widetilde{G}}$$

That is, two graphs are $\sigma$-spectrally similar if their Laplacian matrices are" \cite{spectral-sparse}. Thus, any sparsification that retains $\sigma-$spectral similarity is greatly of interest for the purposes of obtaining a clustering that can directly applied to the original graph. Towards that end, a method dubbed spectral sparsification was procured that seeks to sparsify edges and produce a $\sigma-$spectrally similar graph, with a $\sigma$ tightly bounded from above. This process ``involves assigning a probability $p_{u,v}$ to each edge $(u, v)\in G$ and then selecting edge $(u, v)$ to be in the graph $\widetilde{G}$ with probability $p_{u,v}$. When edge $(u, v)$ is chosen to be in the graph, we multiply its weight by $1/p_{u,v}$. This procedure guarantees that:

$$ \mathbb{E}[L_{\widetilde{G}}] = L_{G} $$" \cite{spectral-sparse}

In fact, the only step of this process that needs further detailing is the assignment of such probabilities $p_{u,v}$. Of course, assigning a lower probability will result in an overall sparser graph, yet doing so also loses information. Thus, the choice of $p_{u,v}$ represents balancing the desire to produce a simpler graph against that of producing one that contains most of the information that was present in the original graph. For this fact, however, a theorem was demonstrated in this same paper:

\begin{theorem}
Suppose $\epsilon\in(0,1/2)$ and $G = (V, E)$ is an unweighted graph with smallest non-zero normalized Laplacian eigenvalue at least $\lambda$. Let $\widetilde{G} = (V, \widetilde{E}, \widetilde{w})$ be a graph obtained by sampling the edges of $G$ with probabilities:

$$ p_e = \min (1,C/\min(d_u, d_v)) $$

for each edge $e = (u,v )$, where: 

$$ C = \Theta ((\log(n))^2 (\epsilon\lambda)^{-2} ) $$

and setting weights $\widetilde{w}_{(e)} = 1/p_e$  for $e\in\widetilde{E}$. Then, with probability at least $1/2$, $\widetilde{G}$ is a $(1 + \epsilon)$-spectral approximation of $G$, and the average degree of $\widetilde{G}$ is $O((\log(n))^2 (\epsilon\lambda)^{−2} )$ \cite{spectral-sparse}
\end{theorem}

Thus, using this theorem, we can employ spectral sparsification on the graphs at hand to obtain a graph that is simpler to deal with for clustering. The sparsification of this matrix substantially reduces the memory consumption of the model, since there are much more efficient methods by which sparse matrices can be represented in most programming paradigms than the naive 2D array representation, making it feasible to deal with the entirety of this dataset when sparsified.

\subsection{Graph Coarsening}
In addition to dealing with space issues, developing a time-efficient means to cluster this data is of great importance. By directly reducing the count of vertices, graph coarsening produces a smaller graph, potentially making certain procedures feasible to run, such as eigendecomposition. Graph coarsening, unlike graph sparsification, seeks to simply clump different pieces of information into a single representation rather than removing unnecessary data. In other words, rather than simply removing vertices from a graph $G$ outright to produce $\widetilde{G}$, the process, alternatively referred to as edge contraction, typically involves coalescing two vertices $u,v$ such that the edges that independently had endpoints at either one now has an endpoint at this single combined vertex. In practice, this procedure is performed on weighted graphs, for which the collection of two edges into a single edge involves adding the weights for the initial two edges to find the resultant weight. Despite being a straightforward procedure, there exist two main variations on this process, referred to as SAG and WAG, respectively depicted in Figures \ref{fig:sag} and  \ref{fig:wag}. Roughly, the former correspond to aggregataing vertices by a strictly enforced threshold value, i.e. coalescing vertices $i,j$ if ``$w_{ij}$ is comparable to $\min\{\max_k(w_{ik}, \max_k(w_{kj})\}$" \cite{coarsening}. The latter corresponds to ``express[ing]  the likelihood of nodes belong[ing] together; these likelihoods will then accumulate at the coarser levels of the process" \cite{coarsening}. 

\begin{figure}
    \label{fig:sag}
    \centering
    \includegraphics[width=0.4\textwidth]{sag.png}
    \caption{SAG (strict aggregation) assumes a strict threshold per which it is determined whether two vertices should be coalesced or not \cite{coarsening}.}
\end{figure}

\begin{figure}
    \label{fig:wag}
    \centering
    \includegraphics[width=0.4\textwidth]{wag.png}
    \caption{WAG (weighted aggregation), in contrast to SAG, determines the \textit{likelihood} for different vertices to be clustered together, whereby vertices can be iteratively aggregated with further levels of coarsening \cite{coarsening}.}
\end{figure}

Having discussed the specifics of the coarsening itself, we consider algorithms that specifically employ this technique. The main one that falls into this category is dubbed METIS, whose overall structure is as follows: ``The graph G is first coarsened down to a few hundred vertices, a bisection of this much smaller graph is computed, and then this partition is projected back towards the original graph (finer graph), by periodically refining the partition" \cite{metis}. In line with that, there are three main facets of this algorithm that necessitate discussion, specifically the coarsening, eventual clustering, and refinement of partitions during uncoarsening.

\begin{figure}
    \label{fig:metis}
    \centering
    \includegraphics[width=0.4\textwidth]{metis.png}
    \caption{METIS relies on continual coarsening of the graph to a point where bisection or clustering is a relatively trivial problem and subsequently refining this partition as the graph is expanded back to its original size \cite{metis}.}
\end{figure}

To be precise, the objective of the coarsening phase is to construct a sequence of graphs $\{G_{l}\}$ such that $|V_{l}| > |V_{l+1}|$, where we denote the original graph $G_{0}=(V_0,E_0)$ \cite{metis}. In line with this, ``$G_{l+1}$ is constructed from $G_l$ by finding a maximal matching $M_l\subset E_l$ of $G_l$ and collapsing together the vertices that are incident on ea ch edge of the matching. In this process no more than two vertices are collapsed together because a matching of a graph is a set of edges, no two of which are incident on the same vertex" \cite{metis}. In line with the difference between SAG and WAG, there were multiple procedures discussed by which the maximal matching could be constructed, though it was found that the HEM ``heavy-edge matching..., [WHICH] computes a matching $M_l$, such that the weight of the edges in $M_l$ is high...produces consistently better results than RM [random matching], and the amount of time spent in refinement is less than that of RM" \cite{metis}.

Having said that, we turn to the clustering section of the algorithm, referred to in Figure \ref{fig:metis} as the ``initial partitioning phase." Specifically, the algorithm supports ``four different schemes for partitioning the coarsest graph...Three of these algorithms are based on graph growing heuristics, and the other one is based on spectral bisection" \cite{metis}. For the purposes of this investigation, we decided to make use of spectral bisection, as this allowed for applying the results of experiments we conducted on the SBM initially. 

From here arises the main spectacle of the METIS algorithm: the uncoarsening phase. Formally, this involves taking the partitioning sets $\{C^k_i\}$ that satisfy $\bigcup_i C_i = V_k$ and projecting them up through $G_{k-1},G_{k-2},...,G_1,G_0$, such that each $\{C^l_i\}$ satisfies $\bigcup_i C^l_i = V_l$. In other words, taking the partitioning of the coarsest graph and attempting to efficiently construct a partitioning that eventually contains all the vertices of the original graph.

Towards that end, we can simply discuss how such a projection happens in a single step of the uncoarsening, from which the full process is fully defined. Specifically, a single step in this process begins with the natural projection up one level. That is to say, if two vertices $u,v$ were coalesced into a supernode $uv$ and said vertex $uv$ was clustered into class $A$, the natural projection simply involves $u,v$ similarly both being clustered into $A$. However, ``even if the partition of $G_l$ is at a local minima, the projected partition of $G_{l−1}$ may not be at a local minima. Since $G_{l−1}$ is finer, it has more degrees of freedom that can be used to further improve the partition and thus decrease the edge-cut. Hence, it may still be possible to improve the projected partition of $G_{l−1}$ by local refinement heuristics" \cite{metis}. In other words, while it is the case that the clustering in the coarser level minimizes the crossing edges, this property is not necessarily retaining in projecting up a level, due to the increased degrees of freedom. A ``refinement heuristic" here refers to one that takes ``two parts of the bisection [$A, B$ and]...selects $A'\subset A$ and $B'\subset B$ such that $A\backslash A' \cup B'$ and $B\backslash B' \cup A'$ is a bisection with a smaller edge-cut" \cite{metis}. The specific algorithm employed for this purpose was the Kernighan-Lin (KL) partition algorithm, though its details are not necessary to understand for our purposes.

In this way, METIS is capable of performing partitioning on enormous graphs, namely by creating a mapping from a large graph to a significantly reduced graph and one from this reduced graph's clustering to that of the original graph. 

\section{Methodology}\label{methodology}


\section{Results}\label{results}

\section{Discussion}\label{discussion}

\section{Conclusion}\label{conclusion}

\clearpage
\begin{thebibliography}{1}
\bibitem{data-stream} Aggarwal, Charu C., et al. ``A Framework for Clustering Evolving Data Streams." Proceedings 2003 VLDB Conference, 2003, pp. 81–92., doi:10.1016/b978-012722442-8/50016-1.
\bibitem{k-means} Arthur, David, and Sergei Vassilvitskii. ``k-Means++: The Advantages of Careful Seeding." \textit{Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms}. Society for Industrial and Applied Mathematics, 2007. 
\bibitem{spectral-sparse} Batson, Joshua, et al. ``Spectral Sparsification of Graphs." Communications of the ACM, vol. 56, no. 8, 2013, p. 87., doi:10.1145/2492007.2492029.
\bibitem{coarsening} Chevalier, Cedric, and Ilya Safro. ``Comparison of Coarsening Schemes for Multilevel Graph Partitioning." Lecture Notes in Computer Science Learning and Intelligent Optimization, 2009, pp. 191–205., doi:10.1007/978-3-642-11169-3\_14.
\bibitem{evolutionary-temporal} Chi, Yun, et al. ``Evolutionary Spectral Clustering by Incorporating Temporal Smoothness." Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '07, 2007, doi:10.1145/1281192.1281212.
\bibitem{silk} Christin, Nicolas. ``Traveling the Silk Road: A Measurement of a Large Anonymous Online Marketplace." 2012, doi:10.21236/ada579383.
\bibitem{scikit} ``Clustering." Scikit-Learn, scikit-learn.org/stable/modules/clustering.html.
\bibitem{coinjoin} ``CoinJoin: Bitcoin Privacy for the Real World." Bitcoin Talk, 22 Aug. 2013, bitcointalk.org/index.php?topic=279249.0.
\bibitem{dbscan} Daszykowski, M., and B. Walczak. ``Density-Based Clustering Methods." Comprehensive Chemometrics, 2009, pp. 635–654., doi:10.1016/b978-044452701-1.00067-3.
\bibitem{wiki-dbscan} ``DBSCAN." \textit{Wikipedia}, Wikimedia Foundation, 19 Apr. 2018, en.wikipedia.org/wiki/DBSCAN. 
\bibitem{eigen-update} Dhanjal, Charanpal, et al. ``Efficient Eigen-Updating for Spectral Graph Clustering." Neurocomputing, vol. 131, 2014, pp. 440–452., doi:10.1016/j.neucom.2013.11.015.
\bibitem{automatic} Ermilov, Dmitry, et al. ``Automatic Bitcoin Address Clustering." 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), 2017, doi:10.1109/icmla.2017.0-118.
\bibitem{evaluation} ``Evaluation of Clustering." Stanford NLP, 2008, nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html.
\bibitem{follow} Greenberg, Andy. ``Follow The Bitcoins: How We Got Busted Buying Drugs On Silk Road's Black Market." Forbes, Forbes Magazine, 7 Sept. 2013, www.forbes.com/sites/andygreenberg/2013/09/05/follow-the-bitcoins-how-we-got-busted-buying-drugs-on-silk-roads-black-market/\#79bb83c8adf7.
\bibitem{bisection} Guattery, Stephen, and Gary L. Miller. ``On the Quality of Spectral Separators." \textit{SIAM Journal on Matrix Analysis and Applications}, vol. 19, no. 3, 1998, pp. 701–719., doi:10.1137/s0895479896312262. 
\bibitem{heuristics} ``Heuristics." \textit{BlockSci 0.4.5 Documentation}, citp.github.io/BlockSci/heuristics/heuristics.html. 
\bibitem{blocksci} Kalodner, Harry. ``BlockSci: Design and Applications of a Blockchain Analysis Platform." ArXiv Preprint, 2017, doi:1709.02489.
\bibitem{metis} Karypis, George, and Vipin Kumar. ``METIS--Unstructured Graph Partitioning and Sparse Matrix Ordering System, Version 2.0." 1995.
\bibitem{goldman} Katz, Lily. ``Goldman Says Cryptocurrencies May Succeed as Form of Real Money." Bloomberg.com, Bloomberg, 10 Jan. 2018, www.bloomberg.com/news/articles/2018-01-10/goldman-says-viability-of-crypto-is-highest-in-developing-world.
\bibitem{mincut} Kothari, Pravesh. ``Karger's Min Cut Algorithm." 2015, Princeton University, Princeton University.
\bibitem{dynamic} LaViers, Amy, et al. ``Dynamic Spectral Clustering." Georgia Institute of Technology, 2010.
\bibitem{sketch} Liberty, Edo. ``Simple and Deterministic Matrix Sketching." Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '13, 2013, doi:10.1145/2487575.2487623.
\bibitem{spectral} Luxburg, Ulrike Von. ``A Tutorial on Spectral Clustering." Statistics and Computing, vol. 17, no. 4, 2007, pp. 395–416., doi:10.1007/s11222-007-9033-z.
\bibitem{fistful} Meiklejohn, Sarah, et al. ``A Fistful of Bitcoins." Proceedings of the 2013 Conference on Internet Measurement Conference - IMC '13, 2013, doi:10.1145/2504730.2504747.
\bibitem{laundering} Moser, Malte, et al. ``An Inquiry into Money Laundering Tools in the Bitcoin Ecosystem." 2013 APWG ECrime Researchers Summit, 2013, doi:10.1109/ecrs.2013.6805780.
\bibitem{sbm} Mossel, Elchanan, et al. ``Reconstruction and Estimation in the Planted Partition Model." Probability Theory and Related Fields, vol. 162, no. 3-4, 2014, pp. 431–461., doi:10.1007/s00440-014-0576-6.
\bibitem{bitcoin} Nakamoto, Satoshi. ``Bitcoin: A Peer-to-Peer Electronic Cash System." 2008.
\bibitem{incremental} Ning, Huazhong, et al. ``Incremental Spectral Clustering by Efficiently Updating the Eigen-System." Pattern Recognition, vol. 43, no. 1, 2010, pp. 113–127., doi:10.1016/j.patcog.2009.06.001.
\bibitem{dynamic-sbm} Pensky, Marianna, and Teng Zhang. ``Spectral Clustering in the Dynamic Stochastic Block Model." ArXiv Preprint, 2017, doi:1705.01204.
\bibitem{fast-coarsen} Purohit, Manish, et al. ``Fast Influence-Based Coarsening for Large Networks." Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '14, 2014, doi:10.1145/2623330.2623701.
\bibitem{spectral-bisection} Rocha, Israel. ``Improvements on Spectral Bisection." ArXiv Preprint, 2017, doi:1703.00268.
\bibitem{cluster-review} Schaeffer, Satu Elisa. ``Graph Clustering." Computer Science Review , 2007, pp. 27–64.
\bibitem{bitcoin-etf} ``Self - Regulatory Organizations; NYSE Arca, Inc; Order Instituting Proceedings to Determine Whether to Approve or Disapprove a Proposed Rule Change to List and Trade the Shares of the ProShares Bitcoin ETF and the ProShares Short Bitcoin ETF under NYSE Arca Rule 8.200 - E, Commentary .02." Securities and Exchange Commission, 23 Mar. 2018.
\bibitem{fast-kmeans} Shindler, Michael, et al. ``Fast and Accurate k-Means For Large Datasets.” Advances in Neural Information Processing Systems, 2011. 
\bibitem{public-key} Shirriff, Ken. ``Ken Shirriff's Blog." Bitcoins the Hard Way: Using the Raw Bitcoin Protocol, www.righto.com/2014/02/bitcoins-hard-way-using-raw-bitcoin.html. 
\bibitem{resistance} Spielman, Daniel A., and Nikhil Srivastava. ``Graph Sparsification by Effective Resistances." Proceedings of the Fourtieth Annual ACM Symposium on Theory of Computing - STOC 08, 2008, doi:10.1145/1374376.1374456.
\bibitem{terry} Tao, Terrence. ``A Review of Probability Theory." \textit{Terry Tao}, terrytao.wordpress.com/2010/01/01/254a-notes-0-a-review-of-probability-theory/.
\bibitem{virtual} Trautman, Lawrence J. ``Virtual Currencies: Bitcoin \& What Now after Liberty Reserve and Silk Road?" SSRN Electronic Journal, 2014, doi:10.2139/ssrn.2393537.
\bibitem{blockchain-img} Wander, Matthaus. ``Bitcoin Block Data." Wikimedia Commons, 22 June 2013, commons.wikimedia.org/wiki/File:Bitcoin\_Block\_Data.png.
\bibitem{scalable} Whang, Joyce Jiyoung, et al. ``Scalable and Memory-Efficient Clustering of Large-Scale Social Networks." 2012 IEEE 12th International Conference on Data Mining, 2012, doi:10.1109/icdm.2012.148.
\bibitem{streaming} Yoo, Shinjae, et al. “Streaming Spectral Clustering.” 2016 IEEE 32nd International Conference on Data Engineering (ICDE), 2016, doi:10.1109/icde.2016.7498277. 
\bibitem{ico} Zetzsche, Dirk A., et al. ``The ICO Gold Rush: It's a Scam, It's a Bubble, It's a Super Challenge for Regulators." SSRN Electronic Journal, 2017, doi:10.2139/ssrn.3072298.
\end{thebibliography}

\end{document}